{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5b9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb52f168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>25.97</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19.71</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.16</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>133.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21.47</td>\n",
       "      <td>80.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4238 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n",
       "0        1   39        4.0              0         0.0     0.0   \n",
       "1        0   46        2.0              0         0.0     0.0   \n",
       "2        1   48        1.0              1        20.0     0.0   \n",
       "3        0   61        3.0              1        30.0     0.0   \n",
       "4        0   46        3.0              1        23.0     0.0   \n",
       "...    ...  ...        ...            ...         ...     ...   \n",
       "4233     1   50        1.0              1         1.0     0.0   \n",
       "4234     1   51        3.0              1        43.0     0.0   \n",
       "4235     0   48        2.0              1        20.0     NaN   \n",
       "4236     0   44        1.0              1        15.0     0.0   \n",
       "4237     0   52        2.0              0         0.0     0.0   \n",
       "\n",
       "      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n",
       "0                   0             0         0    195.0  106.0   70.0  26.97   \n",
       "1                   0             0         0    250.0  121.0   81.0  28.73   \n",
       "2                   0             0         0    245.0  127.5   80.0  25.34   \n",
       "3                   0             1         0    225.0  150.0   95.0  28.58   \n",
       "4                   0             0         0    285.0  130.0   84.0  23.10   \n",
       "...               ...           ...       ...      ...    ...    ...    ...   \n",
       "4233                0             1         0    313.0  179.0   92.0  25.97   \n",
       "4234                0             0         0    207.0  126.5   80.0  19.71   \n",
       "4235                0             0         0    248.0  131.0   72.0  22.00   \n",
       "4236                0             0         0    210.0  126.5   87.0  19.16   \n",
       "4237                0             0         0    269.0  133.5   83.0  21.47   \n",
       "\n",
       "      heartRate  glucose  TenYearCHD  \n",
       "0          80.0     77.0           0  \n",
       "1          95.0     76.0           0  \n",
       "2          75.0     70.0           0  \n",
       "3          65.0    103.0           1  \n",
       "4          85.0     85.0           0  \n",
       "...         ...      ...         ...  \n",
       "4233       66.0     86.0           1  \n",
       "4234       65.0     68.0           0  \n",
       "4235       84.0     86.0           0  \n",
       "4236       86.0      NaN           0  \n",
       "4237       80.0    107.0           0  \n",
       "\n",
       "[4238 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file= pd.read_csv(\"D:\\\\Acmegrade\\\\Files\\\\framingham.csv\")\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e436e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4238 entries, 0 to 4237\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   male             4238 non-null   int64  \n",
      " 1   age              4238 non-null   int64  \n",
      " 2   education        4133 non-null   float64\n",
      " 3   currentSmoker    4238 non-null   int64  \n",
      " 4   cigsPerDay       4209 non-null   float64\n",
      " 5   BPMeds           4185 non-null   float64\n",
      " 6   prevalentStroke  4238 non-null   int64  \n",
      " 7   prevalentHyp     4238 non-null   int64  \n",
      " 8   diabetes         4238 non-null   int64  \n",
      " 9   totChol          4188 non-null   float64\n",
      " 10  sysBP            4238 non-null   float64\n",
      " 11  diaBP            4238 non-null   float64\n",
      " 12  BMI              4219 non-null   float64\n",
      " 13  heartRate        4237 non-null   float64\n",
      " 14  glucose          3850 non-null   float64\n",
      " 15  TenYearCHD       4238 non-null   int64  \n",
      "dtypes: float64(9), int64(7)\n",
      "memory usage: 529.9 KB\n"
     ]
    }
   ],
   "source": [
    "file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2275008",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= file.iloc[:,:-1].values\n",
    "y= file. iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229562a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.  ,  39.  ,   4.  , ...,  26.97,  80.  ,  77.  ],\n",
       "       [  0.  ,  46.  ,   2.  , ...,  28.73,  95.  ,  76.  ],\n",
       "       [  1.  ,  48.  ,   1.  , ...,  25.34,  75.  ,  70.  ],\n",
       "       ...,\n",
       "       [  0.  ,  48.  ,   2.  , ...,  22.  ,  84.  ,  86.  ],\n",
       "       [  0.  ,  44.  ,   1.  , ...,  19.16,  86.  ,    nan],\n",
       "       [  0.  ,  52.  ,   2.  , ...,  21.47,  80.  , 107.  ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "406857c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b18b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "si=SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "x=si.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080aecc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.  ,  39.  ,   4.  , ...,  26.97,  80.  ,  77.  ],\n",
       "       [  0.  ,  46.  ,   2.  , ...,  28.73,  95.  ,  76.  ],\n",
       "       [  1.  ,  48.  ,   1.  , ...,  25.34,  75.  ,  70.  ],\n",
       "       ...,\n",
       "       [  0.  ,  48.  ,   2.  , ...,  22.  ,  84.  ,  86.  ],\n",
       "       [  0.  ,  44.  ,   1.  , ...,  19.16,  86.  ,  78.  ],\n",
       "       [  0.  ,  52.  ,   2.  , ...,  21.47,  80.  , 107.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30c9bb40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x[y==0,0], x[y == 0,1])\n",
    "plt.scatter(x[y == 1,0], x[y == 1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9643d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "x_train, x_test, y_train, y_test= tts(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ccf070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression()\n",
    "lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fe59012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=lr.predict(x_test)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bccc746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8419811320754716"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score as acs\n",
    "acc_1=acs(y_test,a)\n",
    "acc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcb93963",
   "metadata": {},
   "outputs": [],
   "source": [
    "final=lr.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cbf8c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ca9c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8499292118924021"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(y,final)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f0202df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "cm=confusion_matrix(y,final)\n",
    "cr=classification_report(y,final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f03b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3583,   11],\n",
       "       [ 625,   19]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c7b91a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92      3594\n",
      "           1       0.63      0.03      0.06       644\n",
      "\n",
      "    accuracy                           0.85      4238\n",
      "   macro avg       0.74      0.51      0.49      4238\n",
      "weighted avg       0.82      0.85      0.79      4238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a62e685c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "      <th>Predicted Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4233</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>25.97</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>19.71</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.16</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>133.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21.47</td>\n",
       "      <td>80.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4238 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n",
       "0        1   39        4.0              0         0.0     0.0   \n",
       "1        0   46        2.0              0         0.0     0.0   \n",
       "2        1   48        1.0              1        20.0     0.0   \n",
       "3        0   61        3.0              1        30.0     0.0   \n",
       "4        0   46        3.0              1        23.0     0.0   \n",
       "...    ...  ...        ...            ...         ...     ...   \n",
       "4233     1   50        1.0              1         1.0     0.0   \n",
       "4234     1   51        3.0              1        43.0     0.0   \n",
       "4235     0   48        2.0              1        20.0     NaN   \n",
       "4236     0   44        1.0              1        15.0     0.0   \n",
       "4237     0   52        2.0              0         0.0     0.0   \n",
       "\n",
       "      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n",
       "0                   0             0         0    195.0  106.0   70.0  26.97   \n",
       "1                   0             0         0    250.0  121.0   81.0  28.73   \n",
       "2                   0             0         0    245.0  127.5   80.0  25.34   \n",
       "3                   0             1         0    225.0  150.0   95.0  28.58   \n",
       "4                   0             0         0    285.0  130.0   84.0  23.10   \n",
       "...               ...           ...       ...      ...    ...    ...    ...   \n",
       "4233                0             1         0    313.0  179.0   92.0  25.97   \n",
       "4234                0             0         0    207.0  126.5   80.0  19.71   \n",
       "4235                0             0         0    248.0  131.0   72.0  22.00   \n",
       "4236                0             0         0    210.0  126.5   87.0  19.16   \n",
       "4237                0             0         0    269.0  133.5   83.0  21.47   \n",
       "\n",
       "      heartRate  glucose  TenYearCHD  Predicted Output  \n",
       "0          80.0     77.0           0                 0  \n",
       "1          95.0     76.0           0                 0  \n",
       "2          75.0     70.0           0                 0  \n",
       "3          65.0    103.0           1                 0  \n",
       "4          85.0     85.0           0                 0  \n",
       "...         ...      ...         ...               ...  \n",
       "4233       66.0     86.0           1                 0  \n",
       "4234       65.0     68.0           0                 0  \n",
       "4235       84.0     86.0           0                 0  \n",
       "4236       86.0      NaN           0                 0  \n",
       "4237       80.0    107.0           0                 0  \n",
       "\n",
       "[4238 rows x 17 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred=pd.DataFrame(final, columns=[\"Predicted Output\"])\n",
    "result=pd.concat([file,df_pred],axis=1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54ccf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(\"D:\\Acmegrade\\Outputs\\Logistic_Regression.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99f709ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt=DecisionTreeRegressor()\n",
    "dt.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2641e612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=dt.predict(x_test)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "136982ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=dt.predict(x)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "769d3229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9487966021708353"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc=accuracy_score(y,final)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b697d5",
   "metadata": {},
   "source": [
    "# Testing all Algorithms with above dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4bcb3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when i= 0 is 0.9490325625294951\n",
      "Accuracy Score when i= 1 is 0.9495044832468146\n",
      "Accuracy Score when i= 2 is 0.9492685228881548\n",
      "Accuracy Score when i= 3 is 0.9492685228881548\n",
      "Accuracy Score when i= 4 is 0.9492685228881548\n",
      "Accuracy Score when i= 5 is 0.9499764039641341\n",
      "Accuracy Score when i= 6 is 0.9471448796602171\n",
      "Accuracy Score when i= 7 is 0.9471448796602171\n",
      "Accuracy Score when i= 8 is 0.9497404436054743\n",
      "Accuracy Score when i= 9 is 0.9469089193015573\n",
      "Accuracy Score when i= 10 is 0.9504483246814536\n",
      "Accuracy Score when i= 11 is 0.9502123643227938\n",
      "Accuracy Score when i= 12 is 0.9485606418121756\n",
      "Accuracy Score when i= 13 is 0.9495044832468146\n",
      "Accuracy Score when i= 14 is 0.9497404436054743\n",
      "Accuracy Score when i= 15 is 0.950920245398773\n",
      "Accuracy Score when i= 16 is 0.9490325625294951\n",
      "Accuracy Score when i= 17 is 0.9497404436054743\n",
      "Accuracy Score when i= 18 is 0.9485606418121756\n",
      "Accuracy Score when i= 19 is 0.9495044832468146\n",
      "Accuracy Score when i= 20 is 0.9490325625294951\n",
      "Accuracy Score when i= 21 is 0.9483246814535158\n",
      "Accuracy Score when i= 22 is 0.9485606418121756\n",
      "Accuracy Score when i= 23 is 0.9499764039641341\n",
      "Accuracy Score when i= 24 is 0.9504483246814536\n",
      "Accuracy Score when i= 25 is 0.9487966021708353\n",
      "Accuracy Score when i= 26 is 0.9480887210948561\n",
      "Accuracy Score when i= 27 is 0.9511562057574328\n",
      "Accuracy Score when i= 28 is 0.9490325625294951\n",
      "Accuracy Score when i= 29 is 0.9485606418121756\n",
      "Accuracy Score when i= 30 is 0.9485606418121756\n",
      "Accuracy Score when i= 31 is 0.9485606418121756\n",
      "Accuracy Score when i= 32 is 0.9497404436054743\n",
      "Accuracy Score when i= 33 is 0.9506842850401133\n",
      "Accuracy Score when i= 34 is 0.9502123643227938\n",
      "Accuracy Score when i= 35 is 0.9504483246814536\n",
      "Accuracy Score when i= 36 is 0.9485606418121756\n",
      "Accuracy Score when i= 37 is 0.9495044832468146\n",
      "Accuracy Score when i= 38 is 0.9492685228881548\n",
      "Accuracy Score when i= 39 is 0.9490325625294951\n",
      "Accuracy Score when i= 40 is 0.9483246814535158\n",
      "Accuracy Score when i= 41 is 0.9487966021708353\n",
      "Accuracy Score when i= 42 is 0.9483246814535158\n",
      "Accuracy Score when i= 43 is 0.9487966021708353\n",
      "Accuracy Score when i= 44 is 0.9499764039641341\n",
      "Accuracy Score when i= 45 is 0.9466729589428976\n",
      "Accuracy Score when i= 46 is 0.9487966021708353\n",
      "Accuracy Score when i= 47 is 0.9495044832468146\n",
      "Accuracy Score when i= 48 is 0.9492685228881548\n",
      "Accuracy Score when i= 49 is 0.9490325625294951\n",
      "Highest Accuracy is at i= 27 of 0.9511562057574328\n"
     ]
    }
   ],
   "source": [
    "lst=[]\n",
    "for i in range(50):\n",
    "    a=DecisionTreeRegressor(min_samples_split=i)\n",
    "    dt.fit(x_train,y_train)\n",
    "    b=dt.predict(x)\n",
    "    acc=accuracy_score(y,b)\n",
    "    print(\"Accuracy Score when i=\",i,\"is\",acc)\n",
    "    lst.append(acc)\n",
    "max=max(lst)\n",
    "print(\"Highest Accuracy is at i=\",lst.index(max),\"of\",max)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2203ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when i= 1 is 0.11011623928078895\n",
      "Accuracy Score when i= 2 is 0.39713121766090487\n",
      "Accuracy Score when i= 3 is 0.48059596682301187\n",
      "Accuracy Score when i= 4 is 0.5446440992924716\n",
      "Accuracy Score when i= 5 is 0.5721966216986902\n",
      "Accuracy Score when i= 6 is 0.5839986694717405\n",
      "Accuracy Score when i= 7 is 0.5964993344987805\n",
      "Accuracy Score when i= 8 is 0.610189485603162\n",
      "Accuracy Score when i= 9 is 0.6230099202988801\n",
      "Accuracy Score when i= 10 is 0.6366857288026628\n",
      "Accuracy Score when i= 11 is 0.637651487085986\n",
      "Accuracy Score when i= 12 is 0.6312495847499839\n",
      "Accuracy Score when i= 13 is 0.6435221041811736\n",
      "Accuracy Score when i= 14 is 0.6410794691480632\n",
      "Accuracy Score when i= 15 is 0.6511753313263076\n",
      "Accuracy Score when i= 16 is 0.6507512365599412\n",
      "Accuracy Score when i= 17 is 0.659180107790675\n",
      "Accuracy Score when i= 18 is 0.6569293071545215\n",
      "Accuracy Score when i= 19 is 0.6547862097835788\n",
      "Accuracy Score when i= 20 is 0.6645586826042023\n",
      "Accuracy Score when i= 21 is 0.6538510528475318\n",
      "Accuracy Score when i= 22 is 0.6645193001859582\n",
      "Accuracy Score when i= 23 is 0.6688039096812151\n",
      "Accuracy Score when i= 24 is 0.6668785041820717\n",
      "Accuracy Score when i= 25 is 0.6705979904395525\n",
      "Accuracy Score when i= 26 is 0.6653482764845906\n",
      "Accuracy Score when i= 27 is 0.6656964306925729\n",
      "Accuracy Score when i= 28 is 0.6687715613806986\n",
      "Accuracy Score when i= 29 is 0.6711014214185373\n",
      "Accuracy Score when i= 30 is 0.6728425971051362\n",
      "Accuracy Score when i= 31 is 0.6654404715066904\n",
      "Accuracy Score when i= 32 is 0.6723410170683238\n",
      "Accuracy Score when i= 33 is 0.6664335656644973\n",
      "Accuracy Score when i= 34 is 0.6755865753770904\n",
      "Accuracy Score when i= 35 is 0.6703342015992203\n",
      "Accuracy Score when i= 36 is 0.6785993986070301\n",
      "Accuracy Score when i= 37 is 0.6772614183830292\n",
      "Accuracy Score when i= 38 is 0.6768182457697174\n",
      "Accuracy Score when i= 39 is 0.6732388281165229\n",
      "Accuracy Score when i= 40 is 0.6776402943181701\n",
      "Accuracy Score when i= 41 is 0.6739066606226056\n",
      "Accuracy Score when i= 42 is 0.6775984757893752\n",
      "Accuracy Score when i= 43 is 0.6777494515518789\n",
      "Accuracy Score when i= 44 is 0.6765137244936829\n",
      "Accuracy Score when i= 45 is 0.6806229200428067\n",
      "Accuracy Score when i= 46 is 0.6815753030475079\n",
      "Accuracy Score when i= 47 is 0.6769675215298006\n",
      "Accuracy Score when i= 48 is 0.6764850882759127\n",
      "Accuracy Score when i= 49 is 0.6803052899363435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score,r2_score as rs\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "lst1=[]\n",
    "for i in range(1,50):\n",
    "    rf=RandomForestRegressor(n_estimators=i)\n",
    "    rf.fit(x_train,y_train)\n",
    "    a=rf.predict(x)\n",
    "    acc2=rs(y,a)\n",
    "    print(\"Accuracy Score when i=\",i,\"is\",acc2)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb9eecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06945099889885167"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "rf=RandomForestRegressor(n_estimators=3)\n",
    "rf.fit(x_train,y_train)\n",
    "a=rf.predict(x)\n",
    "acc=mse(y,a)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a0e7924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when i= 1 is 0.8822557810287872\n",
      "Accuracy Score when i= 2 is 0.9049079754601227\n",
      "Accuracy Score when i= 3 is 0.927796130250118\n",
      "Accuracy Score when i= 4 is 0.9226050023596036\n",
      "Accuracy Score when i= 5 is 0.9431335535630014\n",
      "Accuracy Score when i= 6 is 0.9334591788579518\n",
      "Accuracy Score when i= 7 is 0.9502123643227938\n",
      "Accuracy Score when i= 8 is 0.9471448796602171\n",
      "Accuracy Score when i= 9 is 0.9532798489853704\n",
      "Accuracy Score when i= 10 is 0.9487966021708353\n",
      "Accuracy Score when i= 11 is 0.9549315714959887\n",
      "Accuracy Score when i= 12 is 0.9535158093440302\n",
      "Accuracy Score when i= 13 is 0.9575271354412459\n",
      "Accuracy Score when i= 14 is 0.9589428975932044\n",
      "Accuracy Score when i= 15 is 0.9608305804624823\n",
      "Accuracy Score when i= 16 is 0.9641340254837187\n",
      "Accuracy Score when i= 17 is 0.9584709768758849\n",
      "Accuracy Score when i= 18 is 0.9615384615384616\n",
      "Accuracy Score when i= 19 is 0.9617744218971213\n",
      "Accuracy Score when i= 20 is 0.9617744218971213\n",
      "Accuracy Score when i= 21 is 0.9608305804624823\n",
      "Accuracy Score when i= 22 is 0.9596507786691836\n",
      "Accuracy Score when i= 23 is 0.9643699858423784\n",
      "Accuracy Score when i= 24 is 0.9627182633317602\n",
      "Accuracy Score when i= 25 is 0.9627182633317602\n",
      "Accuracy Score when i= 26 is 0.9655497876356772\n",
      "Accuracy Score when i= 27 is 0.9634261444077395\n",
      "Accuracy Score when i= 28 is 0.9636621047663992\n",
      "Accuracy Score when i= 29 is 0.9653138272770174\n",
      "Accuracy Score when i= 30 is 0.9643699858423784\n",
      "Accuracy Score when i= 31 is 0.9650778669183577\n",
      "Accuracy Score when i= 32 is 0.9648419065596979\n",
      "Accuracy Score when i= 33 is 0.9657857479943369\n",
      "Accuracy Score when i= 34 is 0.9660217083529967\n",
      "Accuracy Score when i= 35 is 0.9655497876356772\n",
      "Accuracy Score when i= 36 is 0.9657857479943369\n",
      "Accuracy Score when i= 37 is 0.9643699858423784\n",
      "Accuracy Score when i= 38 is 0.9667295894289759\n",
      "Accuracy Score when i= 39 is 0.9662576687116564\n",
      "Accuracy Score when i= 40 is 0.9660217083529967\n",
      "Accuracy Score when i= 41 is 0.9667295894289759\n",
      "Accuracy Score when i= 42 is 0.9662576687116564\n",
      "Accuracy Score when i= 43 is 0.9655497876356772\n",
      "Accuracy Score when i= 44 is 0.9648419065596979\n",
      "Accuracy Score when i= 45 is 0.9657857479943369\n",
      "Accuracy Score when i= 46 is 0.9655497876356772\n",
      "Accuracy Score when i= 47 is 0.9667295894289759\n",
      "Accuracy Score when i= 48 is 0.9672015101462954\n",
      "Accuracy Score when i= 49 is 0.9650778669183577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "lst=[]\n",
    "for i in range(1,50):\n",
    "    rfc=RandomForestClassifier(n_estimators=i)\n",
    "    rfc.fit(x_train,y_train)\n",
    "    a=rfc.predict(x)\n",
    "    acc=accuracy_score(y,a)\n",
    "    print(\"Accuracy Score when i=\",i,\"is\",acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b544379d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when i= 2 is 0.9499764039641341\n",
      "Accuracy Score when i= 3 is 0.9433695139216611\n",
      "Accuracy Score when i= 4 is 0.9367626238791883\n",
      "Accuracy Score when i= 5 is 0.9322793770646531\n",
      "Accuracy Score when i= 6 is 0.9263803680981595\n",
      "Accuracy Score when i= 7 is 0.9193015573383672\n",
      "Accuracy Score when i= 8 is 0.915762151958471\n",
      "Accuracy Score when i= 9 is 0.9096271826333177\n",
      "Accuracy Score when i= 10 is 0.9051439358187825\n",
      "Accuracy Score when i= 11 is 0.9030202925908447\n",
      "Accuracy Score when i= 12 is 0.8954695611137329\n",
      "Accuracy Score when i= 13 is 0.8931099575271354\n",
      "Accuracy Score when i= 14 is 0.8900424728645587\n",
      "Accuracy Score when i= 15 is 0.887918829636621\n",
      "Accuracy Score when i= 16 is 0.8853232656913639\n",
      "Accuracy Score when i= 17 is 0.8829636621047664\n",
      "Accuracy Score when i= 18 is 0.8839075035394054\n",
      "Accuracy Score when i= 19 is 0.8827277017461067\n",
      "Accuracy Score when i= 20 is 0.8829636621047664\n",
      "Accuracy Score when i= 21 is 0.8798961774421897\n",
      "Accuracy Score when i= 22 is 0.8808400188768287\n",
      "Accuracy Score when i= 23 is 0.8810759792354884\n",
      "Accuracy Score when i= 24 is 0.8801321378008494\n",
      "Accuracy Score when i= 25 is 0.8789523360075507\n",
      "Accuracy Score when i= 26 is 0.8782444549315715\n",
      "Accuracy Score when i= 27 is 0.8773006134969326\n",
      "Accuracy Score when i= 28 is 0.8773006134969326\n",
      "Accuracy Score when i= 29 is 0.8765927324209533\n",
      "Accuracy Score when i= 30 is 0.8761208117036338\n",
      "Accuracy Score when i= 31 is 0.8761208117036338\n",
      "Accuracy Score when i= 32 is 0.8754129306276546\n",
      "Accuracy Score when i= 33 is 0.8754129306276546\n",
      "Accuracy Score when i= 34 is 0.8749410099103351\n",
      "Accuracy Score when i= 35 is 0.8702218027371401\n",
      "Accuracy Score when i= 36 is 0.8699858423784804\n",
      "Accuracy Score when i= 37 is 0.8697498820198206\n",
      "Accuracy Score when i= 38 is 0.8699858423784804\n",
      "Accuracy Score when i= 39 is 0.8690420009438414\n",
      "Accuracy Score when i= 40 is 0.8692779613025012\n",
      "Accuracy Score when i= 41 is 0.8695139216611609\n",
      "Accuracy Score when i= 42 is 0.8690420009438414\n",
      "Accuracy Score when i= 43 is 0.8673902784332232\n",
      "Accuracy Score when i= 44 is 0.8673902784332232\n",
      "Accuracy Score when i= 45 is 0.8669183577159038\n",
      "Accuracy Score when i= 46 is 0.866682397357244\n",
      "Accuracy Score when i= 47 is 0.866682397357244\n",
      "Accuracy Score when i= 48 is 0.866682397357244\n",
      "Accuracy Score when i= 49 is 0.866682397357244\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "lst=[]\n",
    "for i in range(2,50):\n",
    "    dtc=DecisionTreeClassifier(min_samples_split=i)\n",
    "    dtc.fit(x_train,y_train)\n",
    "    a=dtc.predict(x)\n",
    "    acc=accuracy_score(y,a)\n",
    "    print(\"Accuracy Score when i=\",i,\"is\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52d76d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score when i= 1 is 0.9516281264747523\n",
      "Accuracy Score when i= 2 is 0.8744690891930156\n",
      "Accuracy Score when i= 3 is 0.8718735252477584\n",
      "Accuracy Score when i= 4 is 0.865738555922605\n",
      "Accuracy Score when i= 5 is 0.8659745162812648\n",
      "Accuracy Score when i= 6 is 0.8560641812175555\n",
      "Accuracy Score when i= 7 is 0.8570080226521944\n",
      "Accuracy Score when i= 8 is 0.8544124587069373\n",
      "Accuracy Score when i= 9 is 0.8574799433695139\n",
      "Accuracy Score when i= 10 is 0.8541764983482775\n",
      "Accuracy Score when i= 11 is 0.8558282208588958\n",
      "Accuracy Score when i= 12 is 0.85181689476168\n",
      "Accuracy Score when i= 13 is 0.8532326569136385\n",
      "Accuracy Score when i= 14 is 0.85181689476168\n",
      "Accuracy Score when i= 15 is 0.85181689476168\n",
      "Accuracy Score when i= 16 is 0.8513449740443606\n",
      "Accuracy Score when i= 17 is 0.8522888154789995\n",
      "Accuracy Score when i= 18 is 0.8506370929683813\n",
      "Accuracy Score when i= 19 is 0.8525247758376593\n",
      "Accuracy Score when i= 20 is 0.8499292118924021\n",
      "Accuracy Score when i= 21 is 0.8499292118924021\n",
      "Accuracy Score when i= 22 is 0.8485134497404436\n",
      "Accuracy Score when i= 23 is 0.8499292118924021\n",
      "Accuracy Score when i= 24 is 0.8494572911750826\n",
      "Accuracy Score when i= 25 is 0.8499292118924021\n",
      "Accuracy Score when i= 26 is 0.8494572911750826\n",
      "Accuracy Score when i= 27 is 0.8492213308164228\n",
      "Accuracy Score when i= 28 is 0.8496932515337423\n",
      "Accuracy Score when i= 29 is 0.8499292118924021\n",
      "Accuracy Score when i= 30 is 0.8494572911750826\n",
      "Accuracy Score when i= 31 is 0.8504011326097216\n",
      "Accuracy Score when i= 32 is 0.8489853704577631\n",
      "Accuracy Score when i= 33 is 0.8501651722510618\n",
      "Accuracy Score when i= 34 is 0.8487494100991033\n",
      "Accuracy Score when i= 35 is 0.8492213308164228\n",
      "Accuracy Score when i= 36 is 0.8494572911750826\n",
      "Accuracy Score when i= 37 is 0.8496932515337423\n",
      "Accuracy Score when i= 38 is 0.8494572911750826\n",
      "Accuracy Score when i= 39 is 0.8499292118924021\n",
      "Accuracy Score when i= 40 is 0.8487494100991033\n",
      "Accuracy Score when i= 41 is 0.8482774893817838\n",
      "Accuracy Score when i= 42 is 0.8485134497404436\n",
      "Accuracy Score when i= 43 is 0.8494572911750826\n",
      "Accuracy Score when i= 44 is 0.8489853704577631\n",
      "Accuracy Score when i= 45 is 0.8489853704577631\n",
      "Accuracy Score when i= 46 is 0.8489853704577631\n",
      "Accuracy Score when i= 47 is 0.8489853704577631\n",
      "Accuracy Score when i= 48 is 0.8487494100991033\n",
      "Accuracy Score when i= 49 is 0.8487494100991033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "lst=[]\n",
    "for i in range(1,50):\n",
    "    knc=KNeighborsClassifier(n_neighbors=i)\n",
    "    knc.fit(x_train,y_train)\n",
    "    a=knc.predict(x)\n",
    "    acc=accuracy_score(y,a)\n",
    "    print(\"Accuracy Score when i=\",i,\"is\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ef265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
